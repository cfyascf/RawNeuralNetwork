{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- * FUNCTIONS * -------------\n",
    "\n",
    "def get_content(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    labels = data['Health_Issue']\n",
    "    features = data.drop('Health_Issue', axis=1)\n",
    "    \n",
    "    # ..transforming categorical values into numeric values..\n",
    "    for column in features.columns:\n",
    "        if(features[column].dtype == 'object'):\n",
    "            features[column] = features[column].astype('category').cat.codes\n",
    "            \n",
    "    testing_samples = int(20 * len(data) / 100)\n",
    "    \n",
    "    testing_labels = labels.iloc[:testing_samples].to_numpy()\n",
    "    testing_features = features.iloc[:testing_samples].to_numpy()\n",
    "    \n",
    "    training_labels = labels.iloc[testing_samples:].to_numpy()\n",
    "    training_features = features.iloc[testing_samples:].to_numpy()\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    # ..the scaler is used to normalize input data so\n",
    "    # it all fit into the same scale, because, it there's numbers\n",
    "    # that are too distinct, the bigger ones have more weight \n",
    "    # in the models result, and it leads to bigger errors..\n",
    "            \n",
    "    return testing_labels, scaler.fit_transform(testing_features), training_labels, scaler.fit_transform(training_features)\n",
    "\n",
    "def xavier_initialization(input_dim, output_dim):\n",
    "    return np.random.randn(output_dim, input_dim) * np.sqrt(2.0 / (input_dim + output_dim))\n",
    "    # ..the weights being too big or too small can\n",
    "    # impact directly the learning process, in this case,\n",
    "    # xavier initialization is applied because is a good\n",
    "    # match for the tanh activation function..\n",
    "    \n",
    "def lecun_initialization(input_dim, output_dim):\n",
    "    stddev = np.sqrt(1.0 / input_dim)\n",
    "    return np.random.normal(0.0, stddev, (output_dim, input_dim))\n",
    "    # lecun initialization is applied because is a good\n",
    "    # match for the relu activation function..\n",
    "\n",
    "def test_model(weights):\n",
    "    sample = random.randrange(0, testing_samples)\n",
    "    \n",
    "    input_biased = np.hstack((bias, testing_features[sample]))\n",
    "        \n",
    "    output1 = np.tanh(weights['weights1'].dot(input_biased))\n",
    "    output1_biased = np.hstack((bias, output1))\n",
    "    \n",
    "    output2 = np.tanh(weights['weights2'].dot(output1_biased))\n",
    "    output2_biased = np.hstack((bias, output2))\n",
    "    \n",
    "    output3 = np.tanh(weights['weights3'].dot(output2_biased))\n",
    "    output3_biased = np.hstack((bias, output3))\n",
    "    \n",
    "    result = np.tanh(weights['weights4'].dot(output3_biased))\n",
    "    \n",
    "    return result, testing_labels[sample]\n",
    "\n",
    "def early_stopping(weights):\n",
    "    testing_times = 100\n",
    "    testing_errors = np.zeros(testing_times)\n",
    "    \n",
    "    for k in range(testing_times):\n",
    "        test_result, test_label = test_model(weights)\n",
    "        error = test_label - test_result\n",
    "        testing_errors[k] = (error * error)/2\n",
    "        \n",
    "    mean_error = testing_errors.mean()\n",
    "        \n",
    "    return True if mean_error < 0.05 else False\n",
    "    # ..using samples the model didn't use for \n",
    "    # training to check it's performance..\n",
    "    \n",
    "def reached_convergence():\n",
    "    if(len(errors_mean) < 500):\n",
    "        return False\n",
    "    \n",
    "    diff = np.diff(errors_mean[-25:])\n",
    "    result = np.all(diff)\n",
    "    \n",
    "    return True if result >= 0 else False\n",
    "    # ..calculates if the lastest 25 values in \n",
    "    # errors array are almost the same, indicating \n",
    "    # they are not getting any lower...\n",
    "\n",
    "def save_weights(filename, weights_dict):    \n",
    "    np.save(filename, weights_dict) \n",
    "        \n",
    "def load_weights(filename):\n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(f\"No weights file found at {filename}\")\n",
    "    \n",
    "    weights = np.load(filename, allow_pickle=True)  \n",
    "    return weights[\"'weights1'\"], weights[\"'weights2'\"], weights[\"'weights3'\"], weights[\"'weights4'\"]\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "# ..if the value is positive, \n",
    "# returns the value..\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "# ..if the value is positive,\n",
    "# returns 1, otherwise, returns 0.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- * CONSTANTS * -------------\n",
    "\n",
    "dataset = 'synthetic_covid_impact_on_work.csv'\n",
    "testing_labels, testing_features, training_labels, training_features = get_content(dataset)\n",
    "\n",
    "training_samples = len(training_features)\n",
    "testing_samples = len(testing_features)\n",
    "\n",
    "epocs = 100000 \n",
    "# ..each epoc represents the time when all\n",
    "# the data has been ran throught, if it's too big\n",
    "# it's probably going to lead your model to overfitting..\n",
    "\n",
    "learning_rate = 0.01\n",
    "# ..keep it low, this value has \n",
    "# a lot of power in progressing the weights..\n",
    "\n",
    "patterns = training_features.shape[1]\n",
    "# ..how many features there is \n",
    "# to train from..\n",
    "\n",
    "bias = 1\n",
    "# ..changes the function's angle..\n",
    "\n",
    "input_neurons = patterns\n",
    "hidden_neurons1 = 82\n",
    "hidden_neurons2 = 128\n",
    "hidden_neurons3 = 64\n",
    "output_neurons = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------- * VARIABLES * -------------\n",
    "\n",
    "# ..the weights matrix need to have 1 column more\n",
    "# because the bias is going to be inserted later on..\n",
    "\n",
    "tm_weights1 = xavier_initialization(input_neurons + 1, hidden_neurons1)\n",
    "tm_weights2 = xavier_initialization(hidden_neurons1 + 1, hidden_neurons2)\n",
    "tm_weights3 = xavier_initialization(hidden_neurons2 + 1, hidden_neurons3)\n",
    "tm_weights4 = xavier_initialization(hidden_neurons3 + 1, output_neurons)\n",
    "# ..weights for the model using hyperbolic \n",
    "# tangent as activation function..\n",
    "\n",
    "rm_weights1 = lecun_initialization(input_neurons + 1, hidden_neurons1)\n",
    "rm_weights2 = lecun_initialization(hidden_neurons1 + 1, hidden_neurons2)\n",
    "rm_weights3 = lecun_initialization(hidden_neurons2 + 1, hidden_neurons3)\n",
    "rm_weights4 = lecun_initialization(hidden_neurons3 + 1, output_neurons)\n",
    "# ..weights for the model using relu\n",
    "# as activation function..\n",
    "\n",
    "errors = np.zeros(training_samples)\n",
    "errors_mean = np.zeros(epocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- * HYPERBOLIC TANGENT MODEL * -------------\n",
    "\n",
    "# ..for each sample in each epoc..\n",
    "for i in range(epocs):\n",
    "    for j in range(training_samples):\n",
    "        \n",
    "        # ..inserting the bias into the inputs, \n",
    "        # multiplying it with it's respective weight \n",
    "        # matrix and applying the activation function..\n",
    "        input_biased = np.hstack((bias, training_features[j]))\n",
    "        \n",
    "        output1 = np.tanh(tm_weights1.dot(input_biased))\n",
    "        output1_biased = np.hstack((bias, output1))\n",
    "        \n",
    "        output2 = np.tanh(tm_weights2.dot(output1_biased))\n",
    "        output2_biased = np.hstack((bias, output2))\n",
    "        \n",
    "        output3 = np.tanh(tm_weights3.dot(output2_biased))\n",
    "        output3_biased = np.hstack((bias, output3))\n",
    "        \n",
    "        result = np.tanh(tm_weights4.dot(output3_biased))\n",
    "        \n",
    "        # ..get the error and make it quadractic so it's more noticeble,\n",
    "        # bigger errors tend to outstand more..\n",
    "        error = training_labels[j] - result\n",
    "        errors[j] = (error ** 2)/2\n",
    "        \n",
    "        # ..calculates the delta for the layer\n",
    "        # and finds the value to adjust the layer behind \n",
    "        # by multiplying it's matrix with the delta..  \n",
    "        delta4 = error * (1 - result ** 2)  \n",
    "        \n",
    "        vdelta3 = tm_weights4.transpose().dot(delta4) \n",
    "        delta3 = vdelta3 * (1 - output3_biased ** 2)\n",
    "        \n",
    "        vdelta2 = tm_weights3.transpose().dot(delta3[1:]) # ..skipping the bias..\n",
    "        delta2 = vdelta2 * (1 - output2_biased ** 2)\n",
    "        \n",
    "        vdelta1 = tm_weights2.transpose().dot(delta2[1:]) # ..skipping the bias..\n",
    "        delta1 = vdelta1 * (1 - output1_biased ** 2)\n",
    "        \n",
    "        # ..adjust the weight's matrixes \n",
    "        # with it's respective delta found..\n",
    "        tm_weights1 += learning_rate * np.outer(delta1[1:], input_biased)\n",
    "        tm_weights2 += learning_rate * np.outer(delta2[1:], output1_biased)\n",
    "        tm_weights3 += learning_rate * np.outer(delta3[1:], output2_biased)\n",
    "        tm_weights4 += learning_rate * np.outer(delta4, output3_biased)\n",
    "        \n",
    "    # ..saving the weights to use it\n",
    "    # later on..\n",
    "    weights = {\n",
    "        'weights1': tm_weights1,\n",
    "        'weights2': tm_weights2,\n",
    "        'weights3': tm_weights3,\n",
    "        'weights4': tm_weights4\n",
    "    }\n",
    "    \n",
    "    save_weights(\"weights_tanh\", weights)\n",
    "            \n",
    "    # ..stopping when reaching such accuracy in \n",
    "    # testing to avoid overfitting..\n",
    "    if(early_stopping(weights)):\n",
    "        print(\"early stopping: model hit 80% or more of accuracy in testing.\")\n",
    "        break\n",
    "    \n",
    "    # ..stopping when the model has reached it's \n",
    "    # convergence state so it won't start losing\n",
    "    # performance..\n",
    "    # if(reached_convergence()):\n",
    "    #     print(\"the model stopped learning because it hit it's convergency point.\")\n",
    "    #     break\n",
    "    \n",
    "    errors_mean[i] = errors.mean()\n",
    "    print(f\"mean error of epoch ({i}): {errors_mean[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- * RELU MODEL * -------------\n",
    "for i in range(epocs):\n",
    "    for j in range(training_samples):\n",
    "        \n",
    "        # ..using relu function as activation this time..\n",
    "        input_biased = np.hstack((bias, training_features[j]))\n",
    "        \n",
    "        output1 = relu(rm_weights1.dot(input_biased))\n",
    "        output1_biased = np.hstack((bias, output1))\n",
    "        \n",
    "        output2 = relu(rm_weights2.dot(output1_biased))\n",
    "        output2_biased = np.hstack((bias, output2))\n",
    "        \n",
    "        output3 = relu(rm_weights3.dot(output2_biased))\n",
    "        output3_biased = np.hstack((bias, output3))\n",
    "        \n",
    "        result = relu(rm_weights4.dot(output3_biased))\n",
    "        \n",
    "        error = training_labels[j] - result\n",
    "        errors[j] = (error ** 2) / 2\n",
    "        \n",
    "        # ..and using the it's derivative to find deltas..\n",
    "        delta4 = error * relu_derivative(result)\n",
    "        \n",
    "        vdelta3 = rm_weights4.transpose().dot(delta4)\n",
    "        delta3 = vdelta3 * relu_derivative(output3_biased)\n",
    "        \n",
    "        vdelta2 = rm_weights3.transpose().dot(delta3[1:])\n",
    "        delta2 = vdelta2 * relu_derivative(output2_biased)\n",
    "        \n",
    "        vdelta1 = rm_weights2.transpose().dot(delta2[1:]) \n",
    "        delta1 = vdelta1 * relu_derivative(output1_biased)\n",
    "        \n",
    "        rm_weights1 += learning_rate * np.outer(delta1[1:], input_biased)\n",
    "        rm_weights2 += learning_rate * np.outer(delta2[1:], output1_biased)\n",
    "        rm_weights3 += learning_rate * np.outer(delta3[1:], output2_biased)\n",
    "        rm_weights4 += learning_rate * np.outer(delta4, output3_biased)\n",
    "        \n",
    "    weights = {\n",
    "        'weights1': rm_weights1,\n",
    "        'weights2': rm_weights2,\n",
    "        'weights3': rm_weights3,\n",
    "        'weights4': rm_weights4\n",
    "    }\n",
    "    \n",
    "    save_weights(\"weights_relu\", weights)\n",
    "            \n",
    "    if(early_stopping(weights)):\n",
    "        print(\"early stopping: model hit 80% or more of accuracy in testing.\")\n",
    "        break\n",
    "    \n",
    "    # if(reached_convergence()):\n",
    "    #     print(\"the model stopped learning because it hit it's convergency point.\")\n",
    "    #     break\n",
    "    \n",
    "    errors_mean[i] = errors.mean()\n",
    "    print(f\"mean error of epoch ({i}): {errors_mean[i]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.17560132]), np.int64(0))\n"
     ]
    }
   ],
   "source": [
    "# print(load_weights(\"weights_tanh.npy\"))\n",
    "print(test_model())\n",
    "# weights1, weights2, weights3, weights4 = load_weights('weights_tanh.npy')\n",
    "\n",
    "\n",
    "# sample = random.randrange(0, training_samples)\n",
    "    \n",
    "# input_biased = np.hstack((bias, training_features[sample]))\n",
    "    \n",
    "# output1 = np.tanh(weights1.dot(input_biased))\n",
    "# output1_biased = np.hstack((bias, output1))\n",
    "\n",
    "# output2 = np.tanh(weights2.dot(output1_biased))\n",
    "# output2_biased = np.hstack((bias, output2))\n",
    "\n",
    "# output3 = np.tanh(weights3.dot(output2_biased))\n",
    "# output3_biased = np.hstack((bias, output3))\n",
    "\n",
    "# result = np.tanh(weights4.dot(output3_biased))\n",
    "\n",
    "# print(result, training_labels[sample])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
