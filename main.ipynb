{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- * FUNCTIONS * -------------\n",
    "\n",
    "def get_content(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    labels = data['Health_Issue']\n",
    "    features = data.drop('Health_Issue', axis=1)\n",
    "    \n",
    "    # ..transforming categorical values into numeric values..\n",
    "    for column in features.columns:\n",
    "        if(features[column].dtype == 'object'):\n",
    "            features[column] = features[column].astype('category').cat.codes\n",
    "            \n",
    "    testing_samples = int(20 * len(data) / 100)\n",
    "    \n",
    "    testing_labels = labels.iloc[:testing_samples].to_numpy()\n",
    "    testing_features = features.iloc[:testing_samples].to_numpy()\n",
    "    \n",
    "    training_labels = labels.iloc[testing_samples:].to_numpy()\n",
    "    training_features = features.iloc[testing_samples:].to_numpy()\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    # ..the scaler is used to normalize input data so\n",
    "    # it all fit into the same scale, because, it there's numbers\n",
    "    # that are too distinct, the bigger ones have more weight \n",
    "    # in the models result, and it leads to bigger errors..\n",
    "            \n",
    "    return testing_labels, scaler.fit_transform(testing_features), training_labels, scaler.fit_transform(training_features)\n",
    "\n",
    "def initialize_weights(input_dim, output_dim):\n",
    "    return np.random.randn(output_dim, input_dim) * np.sqrt(2.0 / (input_dim + output_dim))\n",
    "    # ..the weights being too big or too small can\n",
    "    # impact directly the learning process, in this case,\n",
    "    # xavier initialization is applied because is a good\n",
    "    # match for the tanh activation function..\n",
    "\n",
    "def test_model():\n",
    "    sample = random.randrange(0, testing_samples)\n",
    "    \n",
    "    input_biased = np.hstack((bias, testing_features[sample]))\n",
    "        \n",
    "    output1 = np.tanh(weights1.dot(input_biased))\n",
    "    output1_biased = np.hstack((bias, output1))\n",
    "    \n",
    "    output2 = np.tanh(weights2.dot(output1_biased))\n",
    "    output2_biased = np.hstack((bias, output2))\n",
    "    \n",
    "    output3 = np.tanh(weights3.dot(output2_biased))\n",
    "    output3_biased = np.hstack((bias, output3))\n",
    "    \n",
    "    result = np.tanh(weights4.dot(output3_biased))\n",
    "    \n",
    "    return result, testing_labels[sample]\n",
    "\n",
    "def early_stopping():\n",
    "    testing_times = 100\n",
    "    testing_errors = np.zeros(testing_times)\n",
    "    \n",
    "    for k in range(testing_times):\n",
    "        test_result, test_label = test_model()\n",
    "        error = test_label - test_result\n",
    "        testing_errors[k] = (error * error)/2\n",
    "        \n",
    "    mean_error = testing_errors.mean()\n",
    "        \n",
    "    return True if mean_error < 0.05 else False\n",
    "    # ..using samples the model didn't use for \n",
    "    # training to check it's performance..\n",
    "    \n",
    "def reached_convergence():\n",
    "    if(len(errors_mean) < 500):\n",
    "        return False\n",
    "    \n",
    "    diff = np.diff(errors_mean[-25:])\n",
    "    result = np.all(diff)\n",
    "    \n",
    "    return True if result >= 0 else False\n",
    "    # ..calculates if the lastest 25 values in \n",
    "    # errors array are almost the same, indicating \n",
    "    # they are not getting any lower...\n",
    "\n",
    "def save_weights(filename):\n",
    "    weights_dict = {\n",
    "        'weights1': weights1,\n",
    "        'weights2': weights2,\n",
    "        'weights3': weights3,\n",
    "        'weights4': weights4\n",
    "    }\n",
    "    \n",
    "    np.save(filename, weights_dict) \n",
    "        \n",
    "def load_weights(filename):\n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(f\"No weights file found at {filename}\")\n",
    "    \n",
    "    weights = np.load(filename, allow_pickle=True)  \n",
    "    return weights[\"'weights1'\"], weights[\"'weights2'\"], weights[\"'weights3'\"], weights[\"'weights4'\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- * CONSTANTS * -------------\n",
    "\n",
    "dataset = 'synthetic_covid_impact_on_work.csv'\n",
    "testing_labels, testing_features, training_labels, training_features = get_content(dataset)\n",
    "\n",
    "training_samples = len(training_features)\n",
    "testing_samples = len(testing_features)\n",
    "\n",
    "epocs = 100000 \n",
    "# ..each epoc represents the time when all\n",
    "# the data has been ran throught, if it's too big\n",
    "# it's probably going to lead your model to overfitting..\n",
    "\n",
    "learning_rate = 0.05\n",
    "# ..keep it low, this value has \n",
    "# a lot of power in progressing the weights..\n",
    "\n",
    "patterns = training_features.shape[1]\n",
    "# ..how many features there is \n",
    "# to train from..\n",
    "\n",
    "bias = 1\n",
    "# ..changes the function's angle..\n",
    "\n",
    "input_neurons = patterns\n",
    "hidden_neurons1 = 82\n",
    "hidden_neurons2 = 128\n",
    "hidden_neurons3 = 64\n",
    "output_neurons = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------- * VARIABLES * -------------\n",
    "weights1 = initialize_weights(input_neurons + 1, hidden_neurons1)\n",
    "weights2 = initialize_weights(hidden_neurons1 + 1, hidden_neurons2)\n",
    "weights3 = initialize_weights(hidden_neurons2 + 1, hidden_neurons3)\n",
    "weights4 = initialize_weights(hidden_neurons3 + 1, output_neurons)\n",
    "# ..the weights matrix need to have 1 column more\n",
    "# because the bias is going to be inserted later on..\n",
    "\n",
    "errors = np.zeros(training_samples)\n",
    "errors_mean = np.zeros(epocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yasmim da Cunha\\AppData\\Local\\Temp\\ipykernel_46136\\335276801.py:26: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  errors[j] = (error ** 2)/2\n",
      "C:\\Users\\Yasmim da Cunha\\AppData\\Local\\Temp\\ipykernel_46136\\2826866135.py:61: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  testing_errors[k] = (error * error)/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean error of epoch (0): 0.10925650423263214\n",
      "mean error of epoch (1): 0.1090736224448454\n",
      "mean error of epoch (2): 0.10889982233210647\n",
      "mean error of epoch (3): 0.1088355008962699\n",
      "mean error of epoch (4): 0.108698342142699\n",
      "mean error of epoch (5): 0.10863312257912194\n",
      "mean error of epoch (6): 0.10851433488031668\n",
      "mean error of epoch (7): 0.10847390190284806\n",
      "mean error of epoch (8): 0.10846106865983253\n",
      "mean error of epoch (9): 0.10839619864865821\n",
      "mean error of epoch (10): 0.10834802096519591\n",
      "mean error of epoch (11): 0.10823784824188712\n",
      "mean error of epoch (12): 0.10816881187648085\n",
      "mean error of epoch (13): 0.10814569348686771\n",
      "mean error of epoch (14): 0.10811728273013575\n",
      "mean error of epoch (15): 0.10806947222709744\n",
      "mean error of epoch (16): 0.10803186502774667\n",
      "mean error of epoch (17): 0.1079893723855598\n",
      "mean error of epoch (18): 0.10797980432731782\n",
      "mean error of epoch (19): 0.1079372084162093\n",
      "mean error of epoch (20): 0.10788996635876477\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[262], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m     weights1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mouter(delta1[\u001b[38;5;241m1\u001b[39m:], input_biased)\n\u001b[0;32m     43\u001b[0m     weights2 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mouter(delta2[\u001b[38;5;241m1\u001b[39m:], output1_biased)\n\u001b[1;32m---> 44\u001b[0m     weights3 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mouter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta3\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput2_biased\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     weights4 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mouter(delta4, output3_biased)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# ..saving the weights to use it\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# later on..\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yasmim da Cunha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\numeric.py:983\u001b[0m, in \u001b[0;36mouter\u001b[1;34m(a, b, out)\u001b[0m\n\u001b[0;32m    981\u001b[0m a \u001b[38;5;241m=\u001b[39m asarray(a)\n\u001b[0;32m    982\u001b[0m b \u001b[38;5;241m=\u001b[39m asarray(b)\n\u001b[1;32m--> 983\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnewaxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ------------- * TRAINING * -------------\n",
    "\n",
    "# ..for each sample in each epoc..\n",
    "for i in range(epocs):\n",
    "    for j in range(training_samples):\n",
    "        \n",
    "        # ..inserting the bias into the inputs\n",
    "        # and multiplying it with it's respective\n",
    "        # weight matrix..\n",
    "        input_biased = np.hstack((bias, training_features[j]))\n",
    "        \n",
    "        output1 = np.tanh(weights1.dot(input_biased))\n",
    "        output1_biased = np.hstack((bias, output1))\n",
    "        \n",
    "        output2 = np.tanh(weights2.dot(output1_biased))\n",
    "        output2_biased = np.hstack((bias, output2))\n",
    "        \n",
    "        output3 = np.tanh(weights3.dot(output2_biased))\n",
    "        output3_biased = np.hstack((bias, output3))\n",
    "        \n",
    "        result = np.tanh(weights4.dot(output3_biased))\n",
    "        \n",
    "        # ..get the error and make it quadractic so it's more noticeble,\n",
    "        # bigger errors tend to outstand more..\n",
    "        error = training_labels[j] - result\n",
    "        errors[j] = (error ** 2)/2\n",
    "        \n",
    "        # ..calculates the delta for the layer\n",
    "        # and propagates it for the layer behind..  \n",
    "        delta4 = error * (1 - result ** 2)  \n",
    "        \n",
    "        vdelta3 = weights4.transpose().dot(delta4) \n",
    "        delta3 = vdelta3 * (1 - output3_biased ** 2)\n",
    "        \n",
    "        vdelta2 = weights3.transpose().dot(delta3[1:]) # ..skipping the bias..\n",
    "        delta2 = vdelta2 * (1 - output2_biased ** 2)\n",
    "        \n",
    "        vdelta1 = weights2.transpose().dot(delta2[1:]) # ..skipping the bias..\n",
    "        delta1 = vdelta1 * (1 - output1_biased ** 2)\n",
    "        \n",
    "        # ..adjust the weight's matrix for the next sample..\n",
    "        weights1 += learning_rate * np.outer(delta1[1:], input_biased)\n",
    "        weights2 += learning_rate * np.outer(delta2[1:], output1_biased)\n",
    "        weights3 += learning_rate * np.outer(delta3[1:], output2_biased)\n",
    "        weights4 += learning_rate * np.outer(delta4, output3_biased)\n",
    "        \n",
    "    # ..saving the weights to use it\n",
    "    # later on..\n",
    "    save_weights(\"weights_tanh\")\n",
    "            \n",
    "    # ..stopping when reaching such accuracy in \n",
    "    # testing to avoid overfitting..\n",
    "    if(early_stopping()):\n",
    "        print(\"early stopping: model hit 80% or more of accuracy in testing.\")\n",
    "        break\n",
    "    \n",
    "    # ..stopping when the model has reached it's \n",
    "    # convergence state so it won't start losing\n",
    "    # performance..\n",
    "    # if(reached_convergence()):\n",
    "    #     print(\"the model stopped learning because it hit it's convergency point.\")\n",
    "    #     break\n",
    "    \n",
    "    errors_mean[i] = errors.mean()\n",
    "    print(f\"mean error of epoch ({i}): {errors_mean[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.17560132]), np.int64(0))\n"
     ]
    }
   ],
   "source": [
    "# print(load_weights(\"weights_tanh.npy\"))\n",
    "print(test_model())\n",
    "# weights1, weights2, weights3, weights4 = load_weights('weights_tanh.npy')\n",
    "\n",
    "\n",
    "# sample = random.randrange(0, training_samples)\n",
    "    \n",
    "# input_biased = np.hstack((bias, training_features[sample]))\n",
    "    \n",
    "# output1 = np.tanh(weights1.dot(input_biased))\n",
    "# output1_biased = np.hstack((bias, output1))\n",
    "\n",
    "# output2 = np.tanh(weights2.dot(output1_biased))\n",
    "# output2_biased = np.hstack((bias, output2))\n",
    "\n",
    "# output3 = np.tanh(weights3.dot(output2_biased))\n",
    "# output3_biased = np.hstack((bias, output3))\n",
    "\n",
    "# result = np.tanh(weights4.dot(output3_biased))\n",
    "\n",
    "# print(result, training_labels[sample])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
