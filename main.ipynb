{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- * FUNCTIONS * -------------\n",
    "\n",
    "def get_content(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    labels = data['Health_Issue']\n",
    "    features = data.drop('Health_Issue', axis=1)\n",
    "    \n",
    "    # ..transforming categorical values into numeric values..\n",
    "    for column in features.columns:\n",
    "        if(features[column].dtype == 'object'):\n",
    "            features[column] = features[column].astype('category').cat.codes\n",
    "            \n",
    "    testing_samples = int(20 * len(data) / 100)\n",
    "    \n",
    "    testing_labels = labels.iloc[:testing_samples].to_numpy()\n",
    "    testing_features = features.iloc[:testing_samples].to_numpy()\n",
    "    \n",
    "    training_labels = labels.iloc[testing_samples:].to_numpy()\n",
    "    training_features = features.iloc[testing_samples:].to_numpy()\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    # ..the scaler is used to normalize input data so\n",
    "    # it all fit into the same scale, because, it there's numbers\n",
    "    # that are too distinct, the bigger ones have more weight \n",
    "    # in the models result, and it leads to bigger errors..\n",
    "            \n",
    "    return testing_labels, scaler.fit_transform(testing_features), training_labels, scaler.fit_transform(training_features)\n",
    "\n",
    "def xavier_initialization(input_dim, output_dim):\n",
    "    return np.random.randn(output_dim, input_dim) * np.sqrt(2.0 / (input_dim + output_dim))\n",
    "    # ..the weights being too big or too small can\n",
    "    # impact directly the learning process, in this case,\n",
    "    # xavier initialization is applied because is a good\n",
    "    # match for the tanh activation function..\n",
    "    \n",
    "def lecun_initialization(input_dim, output_dim):\n",
    "    stddev = np.sqrt(1.0 / input_dim)\n",
    "    return np.random.normal(0.0, stddev, (output_dim, input_dim))\n",
    "    # lecun initialization is applied because is a good\n",
    "    # match for the relu activation function..\n",
    "\n",
    "def test_model(weights):\n",
    "    sample = random.randrange(0, testing_samples)\n",
    "    \n",
    "    input_biased = np.hstack((bias, testing_features[sample]))\n",
    "        \n",
    "    output1 = np.tanh(weights['weights1'].dot(input_biased))\n",
    "    output1_biased = np.hstack((bias, output1))\n",
    "    \n",
    "    output2 = np.tanh(weights['weights2'].dot(output1_biased))\n",
    "    output2_biased = np.hstack((bias, output2))\n",
    "    \n",
    "    output3 = np.tanh(weights['weights3'].dot(output2_biased))\n",
    "    output3_biased = np.hstack((bias, output3))\n",
    "    \n",
    "    result = np.tanh(weights['weights4'].dot(output3_biased))\n",
    "    \n",
    "    return result, testing_labels[sample]\n",
    "\n",
    "def early_stopping(weights):\n",
    "    testing_times = 100\n",
    "    testing_errors = np.zeros(testing_times)\n",
    "    \n",
    "    for k in range(testing_times):\n",
    "        test_result, test_label = test_model(weights)\n",
    "        error = test_label - test_result\n",
    "        testing_errors[k] = (error * error)/2\n",
    "        \n",
    "    mean_error = testing_errors.mean()\n",
    "        \n",
    "    return True if mean_error < 0.05 else False\n",
    "    # ..using samples the model didn't use for \n",
    "    # training to check it's performance..\n",
    "    \n",
    "def reached_convergence(errors_mean):\n",
    "    if(np.count_nonzero(errors_mean) < 500):\n",
    "        return False\n",
    "    \n",
    "    diff = np.diff(errors_mean[-250:])\n",
    "    result = np.all(diff)\n",
    "    \n",
    "    return True if result >= 0 else False\n",
    "    # ..calculates if the lastest 25 values in \n",
    "    # errors array are almost the same, indicating \n",
    "    # they are not getting any lower...\n",
    "\n",
    "def save_weights(filename, weights_dict):    \n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(weights_dict, f)\n",
    "        \n",
    "def load_weights(filename):\n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(f\"No weights file found at {filename}\")\n",
    "    \n",
    "    with open(filename, 'rb') as f:\n",
    "        weights = pickle.load(f)\n",
    "\n",
    "    return weights\n",
    "\n",
    "def save_props(filename, props):\n",
    "    np.save(filename, props)\n",
    "\n",
    "def load_props(filename):\n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(f\"No weights file found at {filename}\")\n",
    "    \n",
    "    props = np.load(filename, allow_pickle=True)  \n",
    "    return props\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "# ..if the value is positive, \n",
    "# returns the value..\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "# ..if the value is positive,\n",
    "# returns 1, otherwise, returns 0.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- * CONSTANTS * -------------\n",
    "\n",
    "dataset = 'synthetic_covid_impact_on_work.csv'\n",
    "testing_labels, testing_features, training_labels, training_features = get_content(dataset)\n",
    "\n",
    "training_samples = len(training_features)\n",
    "testing_samples = len(testing_features)\n",
    "\n",
    "epocs = 100000 \n",
    "# ..each epoc represents the time when all\n",
    "# the data has been ran throught, if it's too big\n",
    "# it's probably going to lead your model to overfitting..\n",
    "\n",
    "learning_rate = 0.01\n",
    "# ..keep it low, this value has \n",
    "# a lot of power in progressing the weights..\n",
    "\n",
    "patterns = training_features.shape[1]\n",
    "# ..how many features there is \n",
    "# to train from..\n",
    "\n",
    "bias = 1\n",
    "# ..changes the function's angle..\n",
    "\n",
    "input_neurons = patterns\n",
    "hidden_neurons1 = 82\n",
    "hidden_neurons2 = 128\n",
    "hidden_neurons3 = 64\n",
    "output_neurons = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------- * VARIABLES * -------------\n",
    "\n",
    "# ..the weights matrix need to have 1 column more\n",
    "# because the bias is going to be inserted later on..\n",
    "\n",
    "tm_weights1 = xavier_initialization(input_neurons + 1, hidden_neurons1)\n",
    "tm_weights2 = xavier_initialization(hidden_neurons1 + 1, hidden_neurons2)\n",
    "tm_weights3 = xavier_initialization(hidden_neurons2 + 1, hidden_neurons3)\n",
    "tm_weights4 = xavier_initialization(hidden_neurons3 + 1, output_neurons)\n",
    "# ..weights for the model using hyperbolic \n",
    "# tangent as activation function..\n",
    "\n",
    "rm_weights1 = lecun_initialization(input_neurons + 1, hidden_neurons1)\n",
    "rm_weights2 = lecun_initialization(hidden_neurons1 + 1, hidden_neurons2)\n",
    "rm_weights3 = lecun_initialization(hidden_neurons2 + 1, hidden_neurons3)\n",
    "rm_weights4 = lecun_initialization(hidden_neurons3 + 1, output_neurons)\n",
    "# ..weights for the model using relu\n",
    "# as activation function..\n",
    "\n",
    "errors = np.zeros(training_samples)\n",
    "errors_mean = np.zeros(epocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- * HYPERBOLIC TANGENT MODEL * -------------\n",
    "\n",
    "# ..for each sample in each epoc..\n",
    "for i in range(epocs):\n",
    "    for j in range(training_samples):\n",
    "        \n",
    "        # ..inserting the bias into the inputs, \n",
    "        # multiplying it with it's respective weight \n",
    "        # matrix and applying the activation function..\n",
    "        input_biased = np.hstack((bias, training_features[j]))\n",
    "        \n",
    "        output1 = np.tanh(tm_weights1.dot(input_biased))\n",
    "        output1_biased = np.hstack((bias, output1))\n",
    "        \n",
    "        output2 = np.tanh(tm_weights2.dot(output1_biased))\n",
    "        output2_biased = np.hstack((bias, output2))\n",
    "        \n",
    "        output3 = np.tanh(tm_weights3.dot(output2_biased))\n",
    "        output3_biased = np.hstack((bias, output3))\n",
    "        \n",
    "        result = np.tanh(tm_weights4.dot(output3_biased))\n",
    "        \n",
    "        # ..get the error and make it quadractic so it's more noticeble,\n",
    "        # bigger errors tend to outstand more..\n",
    "        error = training_labels[j] - result\n",
    "        errors[j] = (error ** 2)/2\n",
    "        \n",
    "        # ..calculates the delta for the layer\n",
    "        # and finds the value to adjust the layer behind \n",
    "        # by multiplying it's matrix with the delta..  \n",
    "        delta4 = error * (1 - result ** 2)  \n",
    "        \n",
    "        vdelta3 = tm_weights4.transpose().dot(delta4) \n",
    "        delta3 = vdelta3 * (1 - output3_biased ** 2)\n",
    "        \n",
    "        vdelta2 = tm_weights3.transpose().dot(delta3[1:]) # ..skipping the bias..\n",
    "        delta2 = vdelta2 * (1 - output2_biased ** 2)\n",
    "        \n",
    "        vdelta1 = tm_weights2.transpose().dot(delta2[1:]) # ..skipping the bias..\n",
    "        delta1 = vdelta1 * (1 - output1_biased ** 2)\n",
    "        \n",
    "        # ..adjust the weight's matrixes \n",
    "        # with it's respective delta found..\n",
    "        tm_weights1 += learning_rate * np.outer(delta1[1:], input_biased)\n",
    "        tm_weights2 += learning_rate * np.outer(delta2[1:], output1_biased)\n",
    "        tm_weights3 += learning_rate * np.outer(delta3[1:], output2_biased)\n",
    "        tm_weights4 += learning_rate * np.outer(delta4, output3_biased)\n",
    "        \n",
    "    # ..saving the weights to use it\n",
    "    # later on..\n",
    "    weights = {\n",
    "        'weights1': tm_weights1,\n",
    "        'weights2': tm_weights2,\n",
    "        'weights3': tm_weights3,\n",
    "        'weights4': tm_weights4\n",
    "    }\n",
    "    \n",
    "    save_weights(\"weights_tanh.pkl\", weights)\n",
    "    save_props(\"errors_mean_tanh\", weights)\n",
    "            \n",
    "    # ..stopping when reaching such accuracy in \n",
    "    # testing to avoid overfitting..\n",
    "    if(early_stopping(weights)):\n",
    "        print(\"early stopping: model hit 80% or more of accuracy in testing.\")\n",
    "        break\n",
    "    \n",
    "    # ..stopping when the model has reached it's \n",
    "    # convergence state so it won't start losing\n",
    "    # performance..\n",
    "    if(reached_convergence(errors_mean)):\n",
    "        print(\"the model stopped learning because it hit it's convergency point.\")\n",
    "        break\n",
    "    \n",
    "    errors_mean[i] = errors.mean()\n",
    "    print(f\"mean error of epoch ({i}): {errors_mean[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- * RELU MODEL * -------------\n",
    "for i in range(epocs):\n",
    "    for j in range(training_samples):\n",
    "        \n",
    "        # ..using relu function as activation this time..\n",
    "        input_biased = np.hstack((bias, training_features[j]))\n",
    "        \n",
    "        output1 = relu(rm_weights1.dot(input_biased))\n",
    "        output1_biased = np.hstack((bias, output1))\n",
    "        \n",
    "        output2 = relu(rm_weights2.dot(output1_biased))\n",
    "        output2_biased = np.hstack((bias, output2))\n",
    "        \n",
    "        output3 = relu(rm_weights3.dot(output2_biased))\n",
    "        output3_biased = np.hstack((bias, output3))\n",
    "        \n",
    "        result = relu(rm_weights4.dot(output3_biased))\n",
    "        \n",
    "        error = training_labels[j] - result\n",
    "        errors[j] = (error ** 2) / 2\n",
    "        \n",
    "        # ..and using the it's derivative to find deltas..\n",
    "        delta4 = error * relu_derivative(result)\n",
    "        \n",
    "        vdelta3 = rm_weights4.transpose().dot(delta4)\n",
    "        delta3 = vdelta3 * relu_derivative(output3_biased)\n",
    "        \n",
    "        vdelta2 = rm_weights3.transpose().dot(delta3[1:])\n",
    "        delta2 = vdelta2 * relu_derivative(output2_biased)\n",
    "        \n",
    "        vdelta1 = rm_weights2.transpose().dot(delta2[1:]) \n",
    "        delta1 = vdelta1 * relu_derivative(output1_biased)\n",
    "        \n",
    "        rm_weights1 += learning_rate * np.outer(delta1[1:], input_biased)\n",
    "        rm_weights2 += learning_rate * np.outer(delta2[1:], output1_biased)\n",
    "        rm_weights3 += learning_rate * np.outer(delta3[1:], output2_biased)\n",
    "        rm_weights4 += learning_rate * np.outer(delta4, output3_biased)\n",
    "        \n",
    "    weights = {\n",
    "        'weights1': rm_weights1,\n",
    "        'weights2': rm_weights2,\n",
    "        'weights3': rm_weights3,\n",
    "        'weights4': rm_weights4\n",
    "    }\n",
    "    \n",
    "    save_weights(\"weights_relu.pkl\", weights)\n",
    "    save_props(\"errors_mean_relu\", weights)\n",
    "            \n",
    "    if(early_stopping(weights)):\n",
    "        print(\"early stopping: model hit 80% or more of accuracy in testing.\")\n",
    "        break\n",
    "    \n",
    "    # if(reached_convergence()):\n",
    "    #     print(\"the model stopped learning because it hit it's convergency point.\")\n",
    "    #     break\n",
    "    \n",
    "    errors_mean[i] = errors.mean()\n",
    "    print(f\"mean error of epoch ({i}): {errors_mean[i]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Create line plot\u001b[39;00m\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m---> 10\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinestyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel Accuracy Over Time\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fry1ct\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\matplotlib\\pyplot.py:3794\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3786\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   3787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[0;32m   3788\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3793\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[1;32m-> 3794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3795\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3798\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3799\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3800\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fry1ct\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1779\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1778\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1779\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mc:\\Users\\fry1ct\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\matplotlib\\axes\\_base.py:296\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    295\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fry1ct\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\matplotlib\\axes\\_base.py:483\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    481\u001b[0m     axes\u001b[38;5;241m.\u001b[39mxaxis\u001b[38;5;241m.\u001b[39mupdate_units(x)\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axes\u001b[38;5;241m.\u001b[39myaxis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 483\u001b[0m     \u001b[43maxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myaxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fry1ct\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\matplotlib\\axis.py:1756\u001b[0m, in \u001b[0;36mAxis.update_units\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1754\u001b[0m neednew \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverter \u001b[38;5;241m!=\u001b[39m converter\n\u001b[0;32m   1755\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverter \u001b[38;5;241m=\u001b[39m converter\n\u001b[1;32m-> 1756\u001b[0m default \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_units(default)\n",
      "File \u001b[1;32mc:\\Users\\fry1ct\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\matplotlib\\category.py:105\u001b[0m, in \u001b[0;36mStrCategoryConverter.default_units\u001b[1;34m(data, axis)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# the conversion call stack is default_units -> axis_info -> convert\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis\u001b[38;5;241m.\u001b[39munits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     axis\u001b[38;5;241m.\u001b[39mset_units(\u001b[43mUnitData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m     axis\u001b[38;5;241m.\u001b[39munits\u001b[38;5;241m.\u001b[39mupdate(data)\n",
      "File \u001b[1;32mc:\\Users\\fry1ct\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\matplotlib\\category.py:181\u001b[0m, in \u001b[0;36mUnitData.__init__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counter \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mcount()\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fry1ct\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\matplotlib\\category.py:214\u001b[0m, in \u001b[0;36mUnitData.update\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# check if convertible to number:\u001b[39;00m\n\u001b[0;32m    213\u001b[0m convertible \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m \u001b[43mOrderedDict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# OrderedDict just iterates over unique values in data.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     _api\u001b[38;5;241m.\u001b[39mcheck_isinstance((\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m), value\u001b[38;5;241m=\u001b[39mval)\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convertible:\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;66;03m# this will only be called so long as convertible is True.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "errors_tanh = load_props(\"errors_mean_tanh.npy\")\n",
    "errors_relu = load_props(\"errors_mean_relu.npy\")\n",
    "\n",
    "# Example data\n",
    "epochs = np.arange(1, np.size(errors_tanh)) \n",
    "error = errors_tanh\n",
    "\n",
    "# Create line plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(epochs, error, marker='o', linestyle='-', color='b', label='Accuracy')\n",
    "plt.title('Model Accuracy Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
